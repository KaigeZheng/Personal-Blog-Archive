<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="å¤§æ¨¡å‹å­¦ä¹ ç¬”è®°ï¼ˆä¸‰ï¼‰"><title>Infraå…¥é—¨â€”â€”An Overview of AI Infra</title><link rel=canonical href=https://kaigezheng.github.io/p/llm3/><link rel=stylesheet href=/scss/style.min.f0b427a7e322d7ca4439d64cb0fb1eab709bbe5dc0a5bf10c9071e75f49fced4.css><meta property='og:title' content="Infraå…¥é—¨â€”â€”An Overview of AI Infra"><meta property='og:description' content="å¤§æ¨¡å‹å­¦ä¹ ç¬”è®°ï¼ˆä¸‰ï¼‰"><meta property='og:url' content='https://kaigezheng.github.io/p/llm3/'><meta property='og:site_name' content="Kambri's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='æ–‡æ¡£'><meta property='article:tag' content='AI Infra'><meta property='article:published_time' content='2025-08-15T16:32:00+08:00'><meta property='article:modified_time' content='2025-08-15T16:32:00+08:00'><meta property='og:image' content='https://kaigezheng.github.io/p/llm3/img/cover.jpg'><meta name=twitter:title content="Infraå…¥é—¨â€”â€”An Overview of AI Infra"><meta name=twitter:description content="å¤§æ¨¡å‹å­¦ä¹ ç¬”è®°ï¼ˆä¸‰ï¼‰"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://kaigezheng.github.io/p/llm3/img/cover.jpg'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=åˆ‡æ¢èœå•>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_98fe1fefb25c1393.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>ğŸ« </span></figure><div class=site-meta><h1 class=site-name><a href=/>Kambri's Blog</a></h1><h2 class=site-description>ä½ å¥½ï¼è¿™é‡Œæ˜¯Kambriçš„æŠ€æœ¯&ç”Ÿæ´»åšå®¢ï¼Œæˆ‘å°†åœ¨è¿™é‡Œåˆ†äº«æŠ€æœ¯ç»éªŒå’Œè®°å½•ç”Ÿæ´»ã€‚</h2></div></header><ol class=menu-social><li><a href=https://github.com/KaigeZheng target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=mailto:kambrikg@gmail.com target=_blank title=é‚®ç®±(kambrikg@gmail.com) rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-mail"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M3 7a2 2 0 012-2h14a2 2 0 012 2v10a2 2 0 01-2 2H5a2 2 0 01-2-2V7z"/><path d="M3 7l9 6 9-6"/></svg></a></li><li><a href=https://kaigezheng.github.io/index.xml target=_blank title=RSS rel=me><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-rss"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M5 19m-1 0a1 1 0 102 0 1 1 0 10-2 0"/><path d="M4 4a16 16 0 0116 16"/><path d="M4 11a9 9 0 019 9"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>ä¸»é¡µ|Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>å½’æ¡£|Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>æœç´¢|Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>å‹é“¾|Links</span></a></li><li><a href=/devlog/><svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="icon icon-tabler icons-tabler-outline icon-tabler-logs"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 12h.01"/><path d="M4 6h.01"/><path d="M4 18h.01"/><path d="M8 18h2"/><path d="M8 12h2"/><path d="M8 6h2"/><path d="M14 6h6"/><path d="M14 12h6"/><path d="M14 18h6"/></svg>
<span>æ—¥å¿—|Logs</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>æš—è‰²æ¨¡å¼</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">ç›®å½•</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#model-parameters>Model Parameters</a><ol><li><a href=#parameter-estimation>Parameter Estimation</a></li><li><a href=#computation-estimation>Computation Estimation</a></li></ol></li><li><a href=#memory-estimation>Memory Estimation</a></li><li><a href=#inference-optimization>Inference Optimization</a><ol><li><a href=#kv-cache-optimization>KV Cache OPtimization</a><ol><li><a href=#kv-cache>KV Cache</a></li><li><a href=#mqa>MQA</a></li><li><a href=#gqa>GQA</a></li></ol></li><li><a href=#preliminaries-flashattention>Preliminaries (FlashAttention)</a><ol><li><a href=#online-softmax>Online Softmax</a></li><li><a href=#gpu-memory-architecture>GPU Memory Architecture</a></li><li><a href=#operator>Operator</a></li></ol></li><li><a href=#flashattention>FlashAttention</a><ol><li><a href=#flashattention-v1>FlashAttention-v1</a></li></ol></li></ol></li><li><a href=#training-optimization>Training Optimization</a><ol><li><a href=#parallel-computting-on-data>Parallel Computting (on Data)</a><ol><li><a href=#dp>DP</a></li><li><a href=#ddp>DDP</a></li><li><a href=#fsdp>FSDP</a></li></ol></li><li><a href=#parallel-computting-on-model>Parallel Computting (on Model)</a><ol><li><a href=#tp>TP</a></li><li><a href=#pp>PP</a></li></ol></li></ol></li><li><a href=#quantization>Quantization</a><ol><li><a href=#precision-formats>Precision Formats</a></li></ol></li><li><a href=#reference>Reference</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/llm3/><img src=/p/llm3/img/cover_hu_5de244170c31086e.jpg srcset="/p/llm3/img/cover_hu_5de244170c31086e.jpg 800w, /p/llm3/img/cover_hu_5316e62133ec587e.jpg 1600w" width=800 height=419 loading=lazy alt="Featured image of post Infraå…¥é—¨â€”â€”An Overview of AI Infra"></a></div><div class=article-details><header class=article-category><a href=/categories/%E6%96%87%E6%A1%A3/ style=background-color:#2a9d8f;color:#fff>æ–‡æ¡£
</a><a href=/categories/ai-infra/ style=background-color:#7b79e6;color:#fff>AI Infra</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/llm3/>Infraå…¥é—¨â€”â€”An Overview of AI Infra</a></h2><h3 class=article-subtitle>å¤§æ¨¡å‹å­¦ä¹ ç¬”è®°ï¼ˆä¸‰ï¼‰</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Aug 15, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>é˜…è¯»æ—¶é•¿: 14 åˆ†é’Ÿ</time></div></footer></div></header><section class=article-content><p>è®¡åˆ’åœ¨è¿™ç¯‡åšå®¢é‡Œè°ƒç ”å¹¶ç²—ç•¥åœ°å­¦ä¹ ä¸€ä¸‹åˆ°ç›®å‰ä¸ºæ­¢æ¯”è¾ƒæœ‰å½±å“åŠ›çš„AI Infraå·¥ä½œï¼ˆç±»ä¼¼Surveyï¼‰ï¼Œå¹¶æ…¢æ…¢è¡¥å……ä¸°å¯Œã€‚Anywayï¼Œè¿ˆå‡ºè¡ŒåŠ¨çš„ç¬¬ä¸€æ­¥æœ€éš¾ã€‚</p><h2 id=model-parameters>Model Parameters</h2><h3 id=parameter-estimation>Parameter Estimation</h3><p>1B = 1 Billion = åäº¿</p><p>å‡è®¾æ¨¡å‹å±‚æ•°ä¸º$N$ï¼Œéšè—å±‚ç»´åº¦ä¸º$H$ï¼Œæ¥ä¸‹æ¥è€ƒè™‘ä¸€å±‚Transformerå±‚çš„å‚æ•°é‡ä¼°ç®—ï¼š</p><ul><li>è‡ªæ³¨æ„åŠ›å±‚ï¼šï¼ˆä¸éœ€è¦è€ƒè™‘MHAçš„æƒ…å†µï¼Œå› ä¸ºå¤šå¤´concatèµ·æ¥çš„ä¸ºåº¦å°±ç­‰äºéšè—å±‚ç»´åº¦ï¼‰éœ€è¦æ³¨æ„çš„æ˜¯è¿™é‡ŒåŒ…æ‹¬ä¸€æ¬¡æ³¨æ„åŠ›è®¡ç®—å’Œä¸€æ¬¡çº¿æ€§æ˜ å°„ï¼Œå› æ­¤æ¶‰åŠå››ä¸ªå¯è®­ç»ƒå‚æ•°$W_Q$ã€$W_K$ã€$W_V$å’Œ$W_O$ï¼Œå› æ­¤æ³¨æ„åŠ›å±‚çš„å¯è®­ç»ƒå‚æ•°é‡ä¸º$4H^2 + 4H$ã€‚</li></ul>$$Attention(Q, K, V)=softmax(\frac{QK^{T}}{\sqrt[]{d_{k}} })V \newline Q=W_{Q}X+b_{Q}, K=W_{K}X+b_{K}, V=W_{V}X+b_{V}, O = W_{O}Attention_{O}+b_{O}$$<ul><li>å‰é¦ˆç½‘ç»œå±‚ï¼šFFNå±‚åŒ…æ‹¬ä¸€æ¬¡çº¿æ€§å‡ç»´å’Œä¸€æ¬¡çº¿æ€§é™ç»´ï¼Œè®¾è®¡ä¸¤ä¸ªå¯è®­ç»ƒå‚æ•°$W_{1}$å’Œ$W_{2}$ï¼Œå¯è®­ç»ƒå‚æ•°é‡ä¸º$(H\times 4H + 4H) + (4H \times H + H) = 8H^{2} + 5H$ã€‚</li></ul>$$FFN(x) = GeLU(xW_{1}+b_{1})W_{2}+b_{2}$$<ul><li>æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–ï¼šAdd & Normå±‚ï¼ˆä¸»è¦æ˜¯LNå±‚ï¼‰æ¶‰åŠä¸¤ä¸ªå¯è®­ç»ƒå‘é‡å‚æ•°$\alpha$å’Œ$b$ï¼Œå³2Hã€‚</li></ul>$$Y = Attention(X) + X \newline LN(Y) = \alpha \frac{Y - \mu}{\sigma} + b$$<p>ç»¼ä¸Šï¼Œä¸€å±‚Transformerå±‚ç”±ä¸€å±‚Attentionå±‚ã€ä¸€å±‚FFNå±‚å’Œä¸¤å±‚Add & Normå±‚ç»„æˆï¼Œå¯è®­ç»ƒå‚æ•°é‡ä¸º$12H^{2} + 13H$ï¼Œå¯ä»¥è¿‘ä¼¼ä¸º$12H^{2}$ã€‚</p><h3 id=computation-estimation>Computation Estimation</h3><blockquote><p>AxBå’ŒBxCçš„çŸ©é˜µç›¸ä¹˜ï¼Œæ¯ä¸ªè¾“å‡ºå…ƒç´ éœ€è¦è¿›è¡Œ$n$æ¬¡ä¹˜æ³•å’Œ$n-1$æ¬¡åŠ æ³•ï¼Œ$\approx 2n FLOPs$ï¼›æ•´ä¸ªçŸ©é˜µå…±æœ‰AxCä¸ªè¾“å‡ºå…ƒç´ ï¼Œå› æ­¤æ€»$FLOPs \approx 2ABC$ï¼Œå¯ä»¥è¿‘ä¼¼ä¸º$ABC$ã€‚å› æ­¤ä¼°ç®—å‚æ•°æ—¶ï¼Œä¸»è¦å…³æ³¨çŸ©é˜µä¹˜æˆ–å‘é‡çŸ©é˜µä¹˜çš„ç»´åº¦å³å¯ã€‚æ³¨æ„$W_{Q/K/V}$çš„ç»´åº¦æ˜¯$HxH$ï¼Œè€Œ$Q/K/V$çš„ç»´åº¦æ˜¯$LxH$ã€‚</p></blockquote><p>æ¥ä¸‹æ¥ä¼°ç®—è®¡ç®—é‡ï¼Œç”±äºLayerNormã€Dropoutç­‰è®¡ç®—é‡è¾ƒå°ï¼Œæš‚æ—¶ä¸è€ƒè™‘ã€‚è®¾æ¨¡å‹å±‚æ•°ä¸º$N$ï¼Œéšè—å±‚ç»´åº¦ä¸º$H$ï¼Œæ‰¹é‡å¤§å°ä¸º$B$ï¼Œåºåˆ—é•¿åº¦ä¸º$L$ï¼š</p><ul><li>è‡ªæ³¨æ„åŠ›å±‚</li></ul><ol><li><p>ï¼ˆLHxHH=LHï¼‰çº¿æ€§æŠ•å½±QKVï¼šæ¯ä¸ªæŠ•å½±æ˜¯$H\times H$ï¼Œåº”ç”¨äºæ¯ä¸ªtokenå°±æ˜¯$BLH^{2}$ï¼Œæ€»å…±3ä¸ªçŸ©é˜µï¼Œå› æ­¤$FLOPs=3BLH^{2}$</p></li><li><p>ï¼ˆLHxHL=LLï¼‰Attention Score ($QK^{T}$)ï¼šæ¯ä¸ªtokenå¯¹åº”ä¸€ä¸ª$L\times L$çš„æ³¨æ„åŠ›çŸ©é˜µï¼Œéœ€è¦åš$H$æ¬¡ä¹˜åŠ ï¼Œçº¦ä¸º$BHL^{2}$</p></li><li><p>ï¼ˆâ€”â€”â€”â€”â€”â€”ï¼‰Softmaxå’ŒScalingï¼šSoftmaxæ¶‰åŠå–æŒ‡ã€æ±‚å’Œã€é€å…ƒç´ é™¤å’Œã€æ•°å€¼ç¨³å®šçš„è®¡ç®—ï¼Œè¿™é‡Œåªèƒ½ä¼°è®¡ä¸º$xBL^{2}$ï¼Œç›¸æ¯”SDPAå¯å¿½ç•¥ä¸è®¡</p></li><li><p>ï¼ˆLLxLH=LHï¼‰Attention Outputä¸Vç›¸ä¹˜ï¼šæ˜¾ç„¶æ˜¯$BHL^{2}$</p></li><li><p>ï¼ˆLHxHH=LHï¼‰è¾“å‡ºçº¿æ€§å±‚$W_{O}$ï¼šæ˜¾ç„¶æ˜¯$BLH^{2}$</p></li></ol><p>å› æ­¤ï¼Œè‡ªæ³¨æ„åŠ›å±‚çš„æ€»FLOPsï¼š</p>$$\approx (3BLH^{2})+(2BHL^{2})+(BLH^{2})=4BLH^{2}+2BHL^{2}$$<ul><li>å‰é¦ˆç½‘ç»œå±‚ï¼š</li></ul><ol><li><p>å‡ç»´ï¼ˆ$H$->$4H$ï¼‰ï¼š$FLOPs=BLH(4H)$</p></li><li><p>é™ç»´ï¼ˆ$4H$->$H$ï¼‰ï¼š$FLOPs=BL(4H)H$</p></li></ol><p>ï¼ˆå½“ç„¶è¿˜æœ‰GeLUæ¿€æ´»ï¼Œä¸è¿‡æ˜¯çº¿æ€§çš„è®¡ç®—é‡ï¼Œå¯ä»¥å¿½ç•¥ä¸è®¡ï¼‰å› æ­¤ï¼ŒFFNå±‚çš„æ€»Flopsï¼š</p>$$\approx 8BLH^{2}$$<p>ç»¼ä¸Šï¼Œ$Total\ FLOPs \approx 12BLH^{2} + 2BHL^{2}$$ã€‚ï¼ˆç†è®ºä¸Šåº”è¯¥å†ä¹˜ä»¥2ï¼‰</p><h2 id=memory-estimation>Memory Estimation</h2><h2 id=inference-optimization>Inference Optimization</h2><h3 id=kv-cache-optimization>KV Cache OPtimization</h3><h4 id=kv-cache>KV Cache</h4><p>KV (Key-Value) Cacheæ˜¯ä¸€ç§åœ¨è‡ªå›å½’æ¨¡å‹ï¼ˆå¦‚Decoder of Transformerï¼‰ä¸­å¸¸ç”¨çš„æ¨ç†åŠ é€ŸæŠ€æœ¯ï¼Œé€šè¿‡åœ¨æ¨ç†çš„æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—è¿‡ç¨‹ä¸­ç¼“å­˜å·²è®¡ç®—è¿‡çš„$Key$å’Œ$Value$ï¼Œå‡å°‘é‡å¤çš„$K$ã€$V$ä¸æƒé‡çŸ©é˜µçš„projectionè®¡ç®—ã€‚</p>$$Attention(Q, K, V)=softmax(\frac{QK^{T}}{\sqrt[]{d_{k}} })V$$<p>ä¸ºä»€ä¹ˆå¯ä»¥ç¼“å­˜$K$å’Œ$V$ï¼Ÿç”±äº<strong>Casual Mask</strong>æœºåˆ¶ï¼Œå½“æ¨¡å‹æ¨ç†æ—¶å½“å‰tokenä¸éœ€è¦ä¸ä¹‹åçš„tokenè¿›è¡ŒAttentionè®¡ç®—ï¼Œå› æ­¤åœ¨è®¡ç®—ç¬¬$t$ä¸ªtokençš„$Attention_{t}$æ—¶ï¼Œåªéœ€è¦$Q_{0:t}$ã€$K_{0:t}$å’Œ$V_{0:t}$ã€‚è€ŒDecoderä¸­çš„$Q$éœ€è¦tokenåœ¨embeddingåé€šè¿‡$W_q$æŠ•å½±ï¼Œä½†$K_{0:t-1}$ä¸$V_{0:t-1}$æ¥è‡ªEncoderä¸­ï¼Œä¸”åœ¨è®¡ç®—$Attention_{0:t-1}$æ—¶å·²è¢«è®¡ç®—è¿‡ï¼Œå› æ­¤å¯ä»¥é€šè¿‡ç¼“å­˜å·²è¢«è®¡ç®—è¿‡çš„å†å²$K$ä¸$V$æ¥èŠ‚çœè¿™éƒ¨åˆ†è®¡ç®—ã€‚</p><p>æ¥ä¸‹æ¥å‚è€ƒ<a class=link href=https://zhuanlan.zhihu.com/p/662498827 target=_blank rel=noopener>çŸ¥ä¹@çœ‹å›¾å­¦</a>çš„å…¬å¼æ¨å¯¼ï¼Œ</p><p>è®¡ç®—ç¬¬ä¸€ä¸ªtokenæ—¶çš„Attentionï¼š</p>$$
Attention(Q, K, V) = softmax(\frac{Q_{1}K_{1}^{T}}{\sqrt[]{d}})V_{1}
$$<p>è®¡ç®—ç¬¬äºŒä¸ªtokenæ—¶çš„Attentionï¼ˆçŸ©é˜µç¬¬äºŒè¡Œå¯¹åº”$Attention_{2}$ï¼‰ï¼Œ$softmax(\frac{Q_{1}K_{2}}{\sqrt d})$é¡¹è¢«maskæ‰äº†ï¼š</p>$$
Attention(Q, K, V) = softmax(\frac{Q_{2}[K_{1}, K_{2}]^{T}}{\sqrt[]{d}})[V_{1}, V_{2}] \newline = \begin{pmatrix}
softmax(\frac{Q_{1}K_{1}^{T}}{\sqrt d}) & softmax(-\infty )\\
softmax(\frac{Q_{2}K_{1}^{T}}{\sqrt d}) & softmax(\frac{Q_{2}K_{2}^{T}}{\sqrt d})
\end{pmatrix}[V_{1}, V_{2}] \newline =\begin{pmatrix}
softmax(\frac{Q_{1}K_{1}^{T}}{\sqrt d})V_{1} + 0 \times V_{2} \\
softmax(\frac{Q_{2}K_{1}^{T}}{\sqrt d})V_{1} + softmax(\frac{Q_{2}K_{2}^{T}}{\sqrt d})V_{2}
\end{pmatrix}
$$<p>ä»¥æ­¤ç±»æ¨ï¼ŒAttentionçŸ©é˜µçš„ä¸¥æ ¼ä¸Šä¸‰è§’éƒ¨åˆ†éƒ½è¢«maskæ‰äº†ï¼Œå› æ­¤<strong>è®¡ç®—ç¬¬$t$ä¸ªtokençš„$Attention_{t}$æ—¶ä¸$Q_{1:t-1}$æ— å…³</strong>ï¼š</p>$$
Attention_{1} = softmax(\frac{Q_{1}K_{1}^{T}}{\sqrt[]{d}})V_{1} \newline Attention_{2} = softmax(\frac{Q_{1}[K_{1}, K_{2}]^{T}}{\sqrt[]{d}})[V_{1}, V_{2}] \newline ... \newline Attention_{t} = softmax(\frac{Q_{t}K_{1:t}^{T}}{\sqrt[]{d}})V_{1:t}
$$<p>æºç å®ç°å‚è€ƒ<a class=link href=https://github.com/huggingface/transformers/blob/c962f1515e40521c0b336877a64dc512da4f486d/src/transformers/models/gpt2/modeling_gpt2.py#L269-L360 target=_blank rel=noopener>Huggingfaceçš„GPT2æ¨ç†å®ç°</a>ï¼ŒKV Cacheçš„é€»è¾‘æ ¸å¿ƒæ€è·¯å¦‚ä¸‹ï¼š</p><ul><li><p>å¯¹äº<code>Cross Attention</code>ï¼Œ$Q$æ¥è‡ªdecoderçš„å½“å‰tokenï¼Œ$KV$æ¥è‡ªencoderçš„å…¨éƒ¨è¾“å‡ºã€‚å› æ­¤$KV$é€šå¸¸ä¸å˜ï¼Œåªéœ€ç”Ÿæˆä¸€æ¬¡å¹¶ç¼“å­˜ã€‚</p></li><li><p>å¯¹äº<code>Self Attention</code>ï¼Œ$QKV$éƒ½æ¥è‡ªdecoderçš„å½“å‰tokenï¼Œå› ä¸ºdecoderéœ€è¦çœ‹è¿‡å»æ‰€æœ‰çš„tokenï¼Œå› æ­¤å‰é¢tokençš„$KV$éƒ½éœ€è¦ç¼“å­˜</p></li></ul><blockquote><p>çœ‹æºç å¥½éš¾â€”â€”</p></blockquote><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>        <span class=n>past_key_value</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>Cache</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>cache_position</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>LongTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>head_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_hidden_states</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>encoder_attention_mask</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>FloatTensor</span><span class=p>]</span> <span class=o>=</span> <span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>output_attentions</span><span class=p>:</span> <span class=n>Optional</span><span class=p>[</span><span class=nb>bool</span><span class=p>]</span> <span class=o>=</span> <span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=o>**</span><span class=n>kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>Union</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span> <span class=nb>tuple</span><span class=p>[</span><span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>]],</span> <span class=o>...</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=c1># åˆ¤æ–­æ˜¯å¦æ˜¯Cross Attention</span>
</span></span><span class=line><span class=cl>        <span class=n>is_cross_attention</span> <span class=o>=</span> <span class=n>encoder_hidden_states</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=c1># Cross Attentionä½¿ç”¨cross_attention_cache</span>
</span></span><span class=line><span class=cl>        <span class=c1># Self Attentionä½¿ç”¨self_attention_cache</span>
</span></span><span class=line><span class=cl>        <span class=c1># ç”¨is_updatedè¡¨ç¤ºå½“å‰å±‚çš„KVæ˜¯å¦å·²ç¼“å­˜ (ç”¨äºCross Attention)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>past_key_value</span><span class=p>,</span> <span class=n>EncoderDecoderCache</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>is_updated</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=o>.</span><span class=n>is_updated</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>layer_idx</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>is_cross_attention</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>curr_past_key_value</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=o>.</span><span class=n>cross_attention_cache</span>
</span></span><span class=line><span class=cl>                <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>curr_past_key_value</span> <span class=o>=</span> <span class=n>past_key_value</span><span class=o>.</span><span class=n>self_attention_cache</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>curr_past_key_value</span> <span class=o>=</span> <span class=n>past_key_value</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>is_cross_attention</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># Cross Attention</span>
</span></span><span class=line><span class=cl>            <span class=n>query_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_attn</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_mask</span> <span class=o>=</span> <span class=n>encoder_attention_mask</span>
</span></span><span class=line><span class=cl>            <span class=c1># å°è¯•è·å–KV Cache</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>is_updated</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>key_states</span> <span class=o>=</span> <span class=n>curr_past_key_value</span><span class=o>.</span><span class=n>layers</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>layer_idx</span><span class=p>]</span><span class=o>.</span><span class=n>keys</span>
</span></span><span class=line><span class=cl>                <span class=n>value_states</span> <span class=o>=</span> <span class=n>curr_past_key_value</span><span class=o>.</span><span class=n>layers</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>layer_idx</span><span class=p>]</span><span class=o>.</span><span class=n>values</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span><span class=p>(</span><span class=n>encoder_hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>split_size</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=c1># å˜æ¢æˆMHAçš„shape</span>
</span></span><span class=line><span class=cl>                <span class=n>shape_kv</span> <span class=o>=</span> <span class=p>(</span><span class=o>*</span><span class=n>key_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>key_states</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>shape_kv</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=n>value_states</span> <span class=o>=</span> <span class=n>value_states</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>shape_kv</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># Self Attention</span>
</span></span><span class=line><span class=cl>            <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_attn</span><span class=p>(</span><span class=n>hidden_states</span><span class=p>)</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>split_size</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>shape_kv</span> <span class=o>=</span> <span class=p>(</span><span class=o>*</span><span class=n>key_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>key_states</span> <span class=o>=</span> <span class=n>key_states</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>shape_kv</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>value_states</span> <span class=o>=</span> <span class=n>value_states</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>shape_kv</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>shape_q</span> <span class=o>=</span> <span class=p>(</span><span class=o>*</span><span class=n>query_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>query_states</span> <span class=o>=</span> <span class=n>query_states</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>shape_q</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># æ›´æ–°ç¼“å­˜: å¯åŠ¨KV Cacheï¼Œä¸”æ˜¯Self Attentionï¼Œæˆ–Cross Attentionæ²¡æœ‰ç¼“å­˜è¿‡çš„æƒ…å†µ</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=p>(</span><span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>is_cross_attention</span><span class=p>)</span> <span class=ow>or</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>past_key_value</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>is_cross_attention</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>is_updated</span>
</span></span><span class=line><span class=cl>        <span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>cache_position</span> <span class=o>=</span> <span class=n>cache_position</span> <span class=k>if</span> <span class=ow>not</span> <span class=n>is_cross_attention</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>            <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span> <span class=o>=</span> <span class=n>curr_past_key_value</span><span class=o>.</span><span class=n>update</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>layer_idx</span><span class=p>,</span> <span class=p>{</span><span class=s2>&#34;cache_position&#34;</span><span class=p>:</span> <span class=n>cache_position</span><span class=p>}</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>is_cross_attention</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>past_key_value</span><span class=o>.</span><span class=n>is_updated</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>layer_idx</span><span class=p>]</span> <span class=o>=</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åˆ¤æ–­æ˜¯å¦æ˜¯å› æœæ³¨æ„åŠ› (Casual)</span>
</span></span><span class=line><span class=cl>        <span class=n>is_causal</span> <span class=o>=</span> <span class=n>attention_mask</span> <span class=ow>is</span> <span class=kc>None</span> <span class=ow>and</span> <span class=n>query_states</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>2</span><span class=p>]</span> <span class=o>&gt;</span> <span class=mi>1</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>is_cross_attention</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># é€‰æ‹©æ³¨æ„åŠ›å®ç°æ–¹å¼</span>
</span></span><span class=line><span class=cl>        <span class=c1># [eager/flash_attention_2/sdpa/triton/xformers]</span>
</span></span><span class=line><span class=cl>        <span class=n>using_eager</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span> <span class=o>==</span> <span class=s2>&#34;eager&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>attention_interface</span><span class=p>:</span> <span class=n>Callable</span> <span class=o>=</span> <span class=n>eager_attention_forward</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span> <span class=o>!=</span> <span class=s2>&#34;eager&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attention_interface</span> <span class=o>=</span> <span class=n>ALL_ATTENTION_FUNCTIONS</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>config</span><span class=o>.</span><span class=n>_attn_implementation</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># é€‰æ‹©ç²¾åº¦æå‡(upcast)å’Œé‡æ’(reorder)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>using_eager</span> <span class=ow>and</span> <span class=bp>self</span><span class=o>.</span><span class=n>reorder_and_upcast_attn</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_weights</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>_upcast_and_reordered_attn</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>query_states</span><span class=p>,</span> <span class=n>key_states</span><span class=p>,</span> <span class=n>value_states</span><span class=p>,</span> <span class=n>attention_mask</span><span class=p>,</span> <span class=n>head_mask</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># è°ƒç”¨æ³¨æ„åŠ›è®¡ç®—å‡½æ•°</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>attention_interface</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>query_states</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>key_states</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>value_states</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>attention_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>head_mask</span><span class=o>=</span><span class=n>head_mask</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>dropout</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>attn_dropout</span><span class=o>.</span><span class=n>p</span> <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>training</span> <span class=k>else</span> <span class=mf>0.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>is_causal</span><span class=o>=</span><span class=n>is_causal</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=o>**</span><span class=n>kwargs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># å°†Attentionç»“æœç”¨çº¿æ€§å±‚c_projæŠ•å½±å›åŸå§‹ç»´åº¦</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>*</span><span class=n>attn_output</span><span class=o>.</span><span class=n>shape</span><span class=p>[:</span><span class=o>-</span><span class=mi>2</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>c_proj</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>resid_dropout</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>attn_output</span><span class=p>,</span> <span class=n>attn_weights</span>
</span></span></code></pre></td></tr></table></div></div><p>åŒæ—¶ï¼ŒKV Cacheåœ¨å‡å°‘é‡å¤$KV$è®¡ç®—çš„åŒæ—¶ä¼šå¼•å…¥å¤§é‡çš„Memoryå¼€é”€ï¼Œå¯ä»¥ç²—ç•¥è®¡ç®—ä¸€ä¸‹KV Cacheçš„æ˜¾å­˜å ç”¨ï¼š</p>$$
Memory = 2 \times batch\_size \times seq\_len \times num\_layers \times num\_heads \times head\_dims \times dtype\_size
$$<h4 id=mqa>MQA</h4><p>Multi-Query Attention (MQA)æ˜¯Googleåœ¨2019å¹´äº<a class=link href=https://arxiv.org/abs/1911.02150 target=_blank rel=noopener>ã€ŠFast Transformer Decoding: One Write-Head is All You Needã€‹</a>æå‡ºçš„ä¸€ç§é«˜æ•ˆæ³¨æ„åŠ›æœºåˆ¶ï¼Œæ—¨åœ¨å‡å°‘æ¨ç†è¿‡ç¨‹ä¸­çš„è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚ä¸ä¼ ç»ŸMHAä¸åŒï¼ŒMQAä¿ç•™å¤šä¸ªQueryå¤´ï¼Œä½†æ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«åŒä¸€ç»„Keyå’ŒValueï¼Œè¿™ç§ç»“æ„æ˜¾è‘—å‡å°‘äº†KV Cacheçš„Memoryå¼€é”€ï¼ŒåŒæ—¶ä¿æŒäº†MHAç›¸è¿‘çš„æ€§èƒ½è¡¨ç°ã€‚</p><p>ä¸‹é¢æ˜¯åŸºäºPyTorchçš„ä¸€ä¸ªç®€å•å®ç°ï¼ˆè¿˜æ²¡å®ç°Casual Maskï¼‰ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn.functional</span> <span class=k>as</span> <span class=nn>F</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiQueryAttention</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>embed_dim</span> <span class=o>=</span> <span class=n>embed_dim</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span> <span class=o>=</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>=</span> <span class=n>embed_dim</span> <span class=o>//</span> <span class=n>num_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># å¤šä¸ªQueryå¤´</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># å…±äº«çš„Keyå’ŒValue</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>embed_dim</span><span class=p>,</span> <span class=n>embed_dim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>mask</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>size</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Q: (B, T, num_heads, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>q</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>q_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>num_heads</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span><span class=p>)</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=c1># (B, num_heads, T, head_dim)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># K, V: shared (B, T, head_dim)-&gt;(B, 1, T, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>k</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>k_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>v</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>v_proj</span><span class=p>(</span><span class=n>x</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Attention Score: (B, num_heads, T, T)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>q</span><span class=p>,</span> <span class=n>k</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span> <span class=o>/</span> <span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>head_dim</span> <span class=o>**</span> <span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mask</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>attn_scores</span> <span class=o>=</span> <span class=n>attn_scores</span><span class=o>.</span><span class=n>masked_fill</span><span class=p>(</span><span class=n>mask</span> <span class=o>==</span> <span class=mi>0</span><span class=p>,</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;-inf&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_weights</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>softmax</span><span class=p>(</span><span class=n>attn_scores</span><span class=p>,</span> <span class=n>dim</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Attention output: (B, num_heads, T, head_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>matmul</span><span class=p>(</span><span class=n>attn_weights</span><span class=p>,</span> <span class=n>v</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># åˆå¹¶å¤´-&gt;(B, T, embed_dim)</span>
</span></span><span class=line><span class=cl>        <span class=n>attn_output</span> <span class=o>=</span> <span class=n>attn_output</span><span class=o>.</span><span class=n>transpose</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span><span class=o>.</span><span class=n>view</span><span class=p>(</span><span class=n>B</span><span class=p>,</span> <span class=n>T</span><span class=p>,</span> <span class=n>C</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>out_proj</span><span class=p>(</span><span class=n>attn_output</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=gqa>GQA</h4><p>Grouped-Query Attention (GQA)Googleä¸åœ¨023å¹´äº<a class=link href=https://arxiv.org/abs/2305.13245 target=_blank rel=noopener>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a>æå‡ºçš„ä¸€ç§ä»‹äºMHAå’ŒMQAä¹‹é—´çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©å¤šä¸ªQueryå¤´å…±äº«åŒä¸€ç»„Keyå’ŒValueï¼Œæ—¨åœ¨ä¿ç•™éƒ¨åˆ†è¡¨è¾¾èƒ½åŠ›çš„åŒæ—¶å¤§å¹…å‡å°‘è®¡ç®—å’Œå†…å­˜å¼€é”€ã€‚</p><figure><img src=/p/llm3/img/1.jpg width='600px"'><figcaption><h4>Overview of grouped-query method</h4></figcaption></figure><p>æºç ä¸Šï¼Œåªåœ¨Huggingfaceçš„ä»“åº“é‡Œæ‰¾åˆ°äº†<a class=link href=https://github.com/huggingface/transformers/blob/6dfd561d9cd722dfc09f702355518c6d09b9b4e3/src/transformers/integrations/sdpa_paged.py#L6-L51 target=_blank rel=noopener>sdpa_attention_paged_forward</a>çš„å®ç°ï¼Œçœ‹ä¸Šå»æŒºGQAçš„ã€‚</p><p>æ ¸å¿ƒæ€è·¯æ˜¯ï¼š</p><ul><li><p>å…ˆç”¨<code>repeat_kv</code>å°†KV headå¤åˆ¶<code>num_attention_heads // num_key_value_heads</code>æ¬¡ï¼ˆä»<code>(B, num_key_value_heads, L, D)</code>åˆ°<code>(B, num_attention_heads, L, D)</code>ï¼‰</p></li><li><p>æ”¯æŒKV Cacheçš„SDPA</p></li></ul><h3 id=preliminaries-flashattention>Preliminaries (FlashAttention)</h3><p><strong>FlashAttention</strong>ç”±Tri Daoç­‰åœ¨2022å¹´äºã€ŠFlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awarenessã€‹æå‡ºï¼Œå¹¶åœ¨2023å¹´äºã€ŠFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
ã€‹æå‡ºv2ç‰ˆæœ¬ï¼Œ2024å¹´äºã€ŠFlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precisionã€‹æå‡ºv3ç‰ˆæœ¬ã€‚</p><h4 id=online-softmax>Online Softmax</h4><p><strong>Naive Softmax</strong>æ¶‰åŠä¸¤æ¬¡readï¼ˆéå†æ±‚sumå’Œé€å…ƒç´ é™¤sum(exp)ï¼‰å’Œä¸€æ¬¡writeï¼ˆç»“æœå†™å›ï¼‰ï¼Œæ•°å­¦å…¬å¼å¦‚ä¸‹ï¼š</p>$$softmax(x_{i}) = \frac{e^{x_{i}}}{\Sigma _{j=1}^{n}e^{x_{j}}}$$<p>å¦‚æœ$x$å¤ªå¤§ï¼Œ$e^x$ä¼šä¸Šæº¢ï¼Œè€Œsafe softmaxè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</p><p><strong>Safe Softmax</strong>æ¶‰åŠä¸‰æ¬¡readï¼ˆè¿˜éœ€è¦éå†ä¸€æ¬¡å‡å»maxï¼‰å’Œä¸€æ¬¡writeï¼Œç›®çš„æ˜¯ä¸ºäº†é¿å…æ•°å€¼æº¢å‡ºï¼Œæ•°å­¦å…¬å¼å¦‚ä¸‹ï¼š</p>$$softmax(x_{i}) = \frac{e^{x_{i} - max(x)}}{\Sigma _{j=1}^{n}e^{x_{j} - max(x)}}$$<p>ä½†æ˜¯éœ€è¦å¤šæ¬¡éå†æ•°æ®ï¼Œæ€§èƒ½è¾ƒå·®ï¼Œè€Œonline softmaxè§£å†³äº†è¿™ä¸€é—®é¢˜ã€‚</p><p><strong>Online Softmax</strong>åªéœ€è¦ä¸¤æ¬¡readï¼ˆéå†ä¸€æ¬¡xå¹¶ç»´æŠ¤<strong>æœ€å¤§å€¼</strong>å’Œ<strong>å½’ä¸€åŒ–å› å­</strong>ï¼‰å’Œä¸€æ¬¡writeï¼Œæ ¸å¿ƒæ€è·¯å¦‚ä¸‹ï¼š</p><ul><li><p><strong>åœ¨çº¿ç»´æŠ¤</strong>å˜é‡ï¼ˆ$m_t$ï¼Œå½“å‰å‰$t$ä¸ªå…ƒç´ çš„æœ€å¤§å€¼ï¼›$d_t$ï¼Œå½“å‰å‰$t$ä¸ªå…ƒç´ çš„å½’ä¸€åŒ–å› å­ï¼‰</p></li><li><p>åˆå§‹åŒ–</p><p>$m_0=-\infty, d_0=0$</p></li><li><p>éå†å¹¶ç»´æŠ¤å˜é‡</p></li></ul><ol><li><p>æ›´æ–°æœ€å¤§å€¼ï¼š</p><p>$m_t=max(m_{t-1}, x_t)$</p></li><li><p>æ›´æ–°å½’ä¸€åŒ–å› å­ï¼ˆé€’æ¨ï¼‰ã€éš¾ç‚¹ã€‘ï¼š</p><p>$d_t=d_{t-1}\cdot e^{m_{t-1}-m_t}+e^{x_t - m_t}(=\Sigma_{j=1}^{t-1}e^{x_j-m_{t-1}}\cdot e^{m_{t-1}-m_t}+e^{x_t - m_t})$</p></li></ol><p>è¿™é‡Œçš„å…¬å¼æ¨å¯¼éå¸¸å·§å¦™ï¼Œåº”ç”¨äº†<strong>åŒåº•æŒ‡æ•°ç›¸ä¹˜ç­‰äºä¸¤ä¸ªæŒ‡æ•°å¹‚ç›¸åŠ </strong>ï¼Œè®ºæ–‡çš„æ¨å¯¼å¦‚ä¸‹ï¼Œ<strong>$d_{t}$ä»£è¡¨å‰$t$ä¸ªæ•°ä¸æœ€å¤§å€¼ï¼ˆå±€éƒ¨ï¼Œå³$m_t$ï¼‰ä¹‹å·®çš„æŒ‡æ•°å’Œ</strong>ï¼š</p>$$d_t = d_{t-1}\times e^{m_{t-1}-m_t} + e^{x_t-m_t} \newline =(\Sigma_{j=1}^{t-1}e^{x_j-m_{t-1}}) \times e^{m_{t-1}-m_t} + e^{x_t-m_t} \newline = \Sigma_{j=1}^{t-1}e^{x_j-m_t}+e^{x_t-m_t} \newline = \Sigma_{j=1}^{t}e^{x_j-m_t}$$<blockquote><p>å¯ä»¥è¿™ä¹ˆç†è§£ï¼šæ¯æ¬¡æ›´æ–°å½’ä¸€åŒ–å› å­æ—¶ï¼Œéƒ½ä¹˜ä»¥äº†$e^{m_{t-1} - m_{t}}$ï¼Œé‚£ä¹ˆæœ€åè¿™ä¸ªå› å­ä¼šæ˜¯$e^{0-m_{global}}$ï¼Œæ­£æ˜¯åˆ†æ¯$e^{x_{t}-m_{global}}$çš„ä¸€éƒ¨åˆ†ï¼Œå¦‚æ­¤å·§å¦™åœ°å°†å…¨å±€æœ€å¤§å€¼ä¿ç•™åˆ°äº†éå†ç»“æŸï¼Œè€Œä¸”åœ¨é€’æ¨ä¸­çš„æ¯ä¸€æ­¥éƒ½çº æ­£äº†ä¹‹å‰çš„å±€éƒ¨æœ€å¤§å€¼</p></blockquote><p>online softmaxçš„ä¼ªä»£ç å¦‚ä¸‹ï¼Œå®ç°ä¸Šè¿˜æ˜¯æ¯”è¾ƒç®€å•çš„ï¼š</p><figure><img src=/p/llm3/img/4.jpg width='500px"'><figcaption><h4>Pseudocode of Online Softmax</h4></figcaption></figure><p>å‚è€ƒ@TaurusMoonçš„å®ç°å†™äº†C++çš„online softmax kernelï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=k>using</span> <span class=k>namespace</span> <span class=n>std</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>template</span><span class=o>&lt;</span><span class=k>typename</span> <span class=n>T</span><span class=o>&gt;</span>
</span></span><span class=line><span class=cl><span class=kt>void</span> <span class=n>OnlineSoftmax</span><span class=p>(</span><span class=n>T</span><span class=o>*</span> <span class=n>dst</span><span class=p>,</span> <span class=k>const</span> <span class=n>T</span><span class=o>*</span> <span class=n>src</span><span class=p>,</span> <span class=kt>int</span> <span class=n>n</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>T</span> <span class=n>m</span> <span class=o>=</span> <span class=o>-</span><span class=n>numeric_limits</span><span class=o>&lt;</span><span class=n>T</span><span class=o>&gt;::</span><span class=n>infinity</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=n>T</span> <span class=n>d</span> <span class=o>=</span> <span class=mf>0.0f</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>+</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>T</span> <span class=n>m_update</span> <span class=o>=</span> <span class=n>max</span><span class=p>(</span><span class=n>m</span><span class=p>,</span> <span class=n>src</span><span class=p>[</span><span class=n>i</span><span class=p>]);</span>
</span></span><span class=line><span class=cl>        <span class=n>d</span> <span class=o>=</span> <span class=n>d</span> <span class=o>*</span> <span class=n>exp</span><span class=p>(</span><span class=n>m</span> <span class=o>-</span> <span class=n>m_update</span><span class=p>)</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>src</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>m_update</span><span class=p>);</span>
</span></span><span class=line><span class=cl>        <span class=n>m</span> <span class=o>=</span> <span class=n>m_update</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=kt>int</span> <span class=n>i</span> <span class=o>=</span> <span class=mi>0</span><span class=p>;</span> <span class=n>i</span> <span class=o>&lt;</span> <span class=n>n</span><span class=p>;</span> <span class=o>++</span><span class=n>i</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=n>dst</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=n>src</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=o>-</span> <span class=n>m</span><span class=p>)</span> <span class=o>/</span> <span class=n>d</span><span class=p>;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></td></tr></table></div></div><h4 id=gpu-memory-architecture>GPU Memory Architecture</h4><ul><li><p>æ˜¾å­˜/é«˜å¸¦å®½å†…å­˜ï¼ˆ<strong>HBM</strong>, High Bandwidth Memoryï¼‰æ˜¯å°è£…åœ¨GPU Coreå¤–çš„DRAMï¼ˆåŠ¨æ€å­˜å‚¨ï¼Œéœ€è¦å‘¨æœŸæ€§åˆ·æ–°ï¼‰ï¼Œé€šè¿‡è¶…å®½æ€»çº¿è¿æ¥GPU Coreï¼Œå¤§å®¹é‡çš„åŒæ—¶å»¶è¿Ÿä¹Ÿç›¸å¯¹è¾ƒå¤§ã€‚</p></li><li><p>é™æ€å†…å­˜ï¼ˆ<strong>SRAM</strong>, Static Random Access Memoryï¼‰**æ˜¯å°è£…åœ¨GPU Coreå†…éƒ¨çš„SRAMï¼ˆé™æ€å­˜å‚¨ï¼‰ï¼Œå¦‚Registerã€Shared Memoryã€L1/L2 Cacheã€‚</p></li></ul><figure><img src=/p/llm3/img/5.jpg width='300px"'><figcaption><h4>Memory/Bandwidth Architcture of A100</h4></figcaption></figure><h4 id=operator>Operator</h4><p>ç®—å­ä¸»è¦å¯ä»¥åˆ†ä¸ºä¸¤ç±»ï¼š</p><ul><li><p><strong>è®¡ç®—å—é™å‹</strong>ï¼šå¦‚GEMMç­‰</p></li><li><p><strong>å†…å­˜å—é™å‹</strong>ï¼šä¸»è¦æ˜¯element-wiseç±»ï¼ˆå¦‚Activationã€Dropoutã€Maskibgç­‰ï¼‰å’Œreductionç±»ï¼ˆå¦‚Sumã€Softmaxã€LayerNormç­‰ï¼‰</p></li></ul><h3 id=flashattention>FlashAttention</h3><h4 id=flashattention-v1>FlashAttention-v1</h4><p>Transformerçš„æ ¸å¿ƒè®¡ç®—æ˜¯Attentionï¼Œæœ´ç´ çš„Attentionè®¡ç®—æ­¥éª¤å¦‚ä¸‹ï¼Œå…¶ä¸­ä¸€èˆ¬$N \gg d$ï¼Œå¤æ‚åº¦æ˜¯$N^2$ï¼Œåœ¨é•¿åºåˆ—é¢‘ç¹è¯»å†™ï¼ˆ5read & 3 writeï¼‰å¤§çŸ©é˜µæ—¶éå¸¸ä¾èµ–HBMï¼š</p><figure><img src=/p/llm3/img/6.jpg width='500px"'><figcaption><h4>Standard Attention Implementation</h4></figcaption></figure><p>FlashAttentionçš„æ ¸å¿ƒæ€è·¯å°±æ˜¯æé«˜Attentionç®—å­çš„SRAMåˆ©ç”¨ç‡ï¼ˆå°†è¾“å…¥çš„QKVçŸ©é˜µä»HBMåŠ è½½åˆ°SRAMä¸­è®¡ç®—ï¼‰ï¼Œå‡å°‘HBMè®¿å­˜ã€‚</p><ul><li><strong>Tiling</strong></li></ul><p>å¸¸è§„çš„row-wise softmaxä¸é€‚åˆåˆ†å—çš„ç®—æ³•ï¼Œå› æ­¤è¿™é‡Œéœ€è¦ä½¿ç”¨online softmaxï¼Œåœ¨åˆ†å—åçš„èŒƒå›´å†…ï¼Œç‰‡ä¸Šè®¡ç®—maxå’Œrowsumï¼Œå¹¶åœ¨é€šä¿¡åè®¡ç®—å…¨å±€çš„maxå’Œscale factorã€‚</p><ul><li><strong>Recomputation</strong></li></ul><p>åœ¨åå‘ä¼ æ’­çš„ä¼˜åŒ–ï¼Œè®¡ç®—æ¢¯åº¦éœ€è¦ç”¨åˆ°QKè®¡ç®—çš„attention score ($S$)å’Œsoftmaxåçš„attention score ($P$)ã€‚FlashAttentioné€šè¿‡å­˜å‚¨Attentionçš„è¾“å‡ºç»“æœ($O$)å’Œå½’ä¸€åŒ–ç»Ÿè®¡é‡$(m, l)$æ¥å¿«é€Ÿè®¡ç®—$S$å’Œ$P$ï¼Œé¿å…äº†ç”¨$QKV$çš„é‡å¤è®¡ç®—ã€‚</p><ul><li><strong>Kernel Fusion</strong></li></ul><p>å¾ˆå¸¸è§çš„ä¼˜åŒ–ï¼Œå‡å°‘äº†å¤šä½™çš„HBMå†™å›å’Œé‡æ–°åŠ è½½ã€‚</p><figure><img src=/p/llm3/img/7.jpg width='300px"'><figcaption><h4>PyTorch vs. FlashAttention on GPT-2</h4></figcaption></figure><p>æ€»ç»“ä¸€ä¸‹ï¼ŒFlashAttentionå¯ä»¥è®©è®¡ç®—æé€Ÿ2-4å€ï¼ŒèŠ‚çº¦10å€ä»¥ä¸Šå†…å­˜ï¼ˆä¸»è¦æ˜¯è¾¹å­˜å˜ç®—ï¼Œä¸ç”¨å­˜å‚¨å¤æ‚åº¦ä¸º$N^2$çš„$QKV$ï¼Œè½¬è€Œå­˜å‚¨å¤æ‚åº¦ä¸º$Nd$çš„è¾“å‡ºç»“æœå’Œç»Ÿè®¡é‡ï¼‰ã€‚</p><h2 id=training-optimization>Training Optimization</h2><h3 id=parallel-computting-on-data>Parallel Computting (on Data)</h3><h4 id=dp>DP</h4><p><strong>æ•°æ®å¹¶è¡Œï¼ˆDP, Data Parallelï¼‰</strong>ï¼šæ¨¡å‹å‰¯æœ¬åœ¨æ¯ä¸ªGPUä¸Šå„è‡ªç‹¬ç«‹åœ°å‰å‘ä¼ æ’­ï¼Œæ¢¯åº¦ä¼šèšåˆï¼ˆAllReduceï¼‰åˆ°ä¸»GPUè¿›è¡Œå‚æ•°æ›´æ–°ã€‚ç¼ºç‚¹æ˜¯éè·¨è¿›ç¨‹ï¼Œåªæ”¯æŒå•æœºå¤šå¡ï¼›æ¢¯åº¦èšåˆä¼šå‘ç”Ÿåœ¨ä¸»è®¾å¤‡ï¼Œå¯¼è‡´é€šä¿¡ç“¶é¢ˆå’Œè´Ÿè½½ä¸å‡è¡¡ã€‚</p><p>å®ç°ä¸Šè¾ƒä¸ºç®€å•:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=o>...</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>DataParallel</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>])</span>
</span></span></code></pre></td></tr></table></div></div><p>ä¸è¿‡PyTorchå»ºè®®å¤šå¡å¹¶è¡Œçš„æ—¶å€™ä½¿ç”¨DPPï¼Œå³ä½¿åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼ˆDPçš„æ€§èƒ½è¾ƒDDPæ›´å·®ï¼Œå› ä¸ºä¸»å¡è´Ÿè½½å¾ˆä¸å‡è¡¡ï¼Œå•è¿›ç¨‹å¤šçº¿ç¨‹ç¯å¢ƒä¸‹è®¾è®¡GILç«äº‰ï¼Œä¸”å¯æ‰©å±•æ€§ä¸å¦‚DDPï¼‰ï¼Œæºç å®ç°è§<a class=link href=https://github.com/pytorch/pytorch/blob/978e3a91421e82fc95b34e75efd6324e3e89e755/torch/nn/parallel/data_parallel.py#L53 target=_blank rel=noopener>è¿™é‡Œ</a>ï¼Œæ›´åŠ åº•å±‚çš„Operatoråœ¨<code>torch.nn.parallel.scatter_gather/_functions/comm</code>ä¸‹ï¼ˆå¦‚scatterã€gatherç­‰ï¼‰ã€‚</p><h4 id=ddp>DDP</h4><p><strong>åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼ˆDDP, Distributed Data Parallelï¼‰</strong>ï¼šæ¯ä¸ªè¿›ç¨‹å¯¹åº”ä¸€ä¸ªGPUï¼Œæ¯ä¸ªGPUä¸Šéƒ½æœ‰æ¨¡å‹å‰¯æœ¬ï¼Œæ¢¯åº¦é€šè¿‡AllReduceåŒæ­¥ï¼Œæ¯ä¸ªé‡‘å±‚éƒ½å‚ä¸å‚æ•°æ›´æ–°ï¼ˆæ¯ä¸ªGPUç‹¬ç«‹è¿›è¡Œå‰å‘ã€è®¡ç®—lossã€è®¡ç®—æ¢¯åº¦ï¼Œå¹¶åœ¨AllReduceåé€šè¿‡å¹³å‡æ¢¯åº¦æ›´æ–°ï¼‰ã€‚</p><p>å®ç°ä¸Šå¯ä»¥é€šè¿‡æ‰‹åŠ¨è®¾ç½®å¹¶è¡Œï¼ˆå¤šä¸ªterminalè®¾ç½®RANKã€WORLD_SIZEã€MASTER_ADDRã€MASTER_PORTç­‰ç¯å¢ƒå˜é‡å¹¶å¯åŠ¨è„šæœ¬ï¼‰æˆ–ç”¨<code>torchrun</code>è‡ªåŠ¨ç®¡ç†ç¯å¢ƒå˜é‡ï¼ˆ<code>torchrun --nproc_per_node=... &lt;script></code>ï¼‰ï¼š</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.distributed</span> <span class=k>as</span> <span class=nn>dist</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn.parallel</span> <span class=kn>import</span> <span class=n>DistributedDataParallel</span> <span class=k>as</span> <span class=n>DDP</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;MASTER_ADDR&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;localhost&#39;</span>
</span></span><span class=line><span class=cl><span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;MASTER_PORT&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;...&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dist</span><span class=o>.</span><span class=n>init_process_group</span><span class=p>(</span><span class=n>backend</span><span class=o>=</span><span class=s1>&#39;nccl&#39;</span><span class=p>,</span> <span class=n>rank</span><span class=o>=</span><span class=n>rank</span><span class=p>,</span> <span class=n>world_size</span><span class=o>=</span><span class=n>world_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=o>...</span><span class=p>()</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>DDP</span><span class=p>(</span><span class=n>model</span><span class=p>,</span> <span class=n>device_ids</span><span class=o>=</span><span class=p>[</span><span class=n>rank</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl><span class=n>dist</span><span class=o>.</span><span class=n>destroy_process_group</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p>æºç å‚è€ƒ<a class=link href=https://github.com/pytorch/pytorch/blob/978e3a91421e82fc95b34e75efd6324e3e89e755/torch/nn/parallel/distributed.py#L327 target=_blank rel=noopener>è¿™é‡Œ</a>ã€‚DDPå¯ä»¥ä½¿ç”¨é«˜æ•ˆçš„é€šä¿¡åç«¯ï¼ˆå¦‚NCCLï¼‰ï¼Œæ²¡æœ‰ä¸»ä»ç“¶é¢ˆï¼ˆæ”¯æŒå•æœºå¤šå¡/å¤šæœºå¤šå¡ï¼‰ï¼Œè¿˜æ˜¯éå¸¸å®ç”¨çš„ã€‚</p><h4 id=fsdp>FSDP</h4><p><strong>å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼ˆFSDP, Fully Sharded Data Parallelï¼‰</strong>ï¼šæ¨¡å‹æƒé‡æŒ‰å‚æ•°ç»´åº¦åˆ‡åˆ†åˆ°å¤šä¸ªGPUä¸Šï¼ˆshardï¼‰ï¼Œå‰å‘ä¼ æ’­æ—¶é‡æ–°èšåˆå‚æ•°ï¼ˆgatherï¼‰ï¼Œåå‘ä¼ æ’­åå†åˆ‡åˆ†ï¼ˆreshardï¼‰ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨ï¼Œä¸»è¦é€šè¿‡<code>torch.nn.distributed.fsdp.FullyShardedDataParallel</code>æ¥å®ç°ï¼Œæºç å‚è€ƒ<a class=link href=https://github.com/pytorch/pytorch/blob/978e3a91421e82fc95b34e75efd6324e3e89e755/torch/distributed/fsdp/fully_sharded_data_parallel.py#L116 target=_blank rel=noopener>è¿™é‡Œ</a>ã€‚</p><blockquote><p>æœ¬è´¨ä¸ŠFSDPè¿˜æ˜¯æ•°æ®å¹¶è¡Œï¼ŒçŸ¥è¯†å‚æ•°åˆ†åˆ«æœ‰ç‚¹æ¨¡å‹å¹¶è¡Œçš„å‘³é“</p></blockquote><h3 id=parallel-computting-on-model>Parallel Computting (on Model)</h3><figure><img src=/p/llm3/img/2.jpg width='600px"'><figcaption><h4>Existing parallelism for distributed training (sorryæˆ‘æ²¡æ‰¾åˆ°å›¾ç‰‡æ¥æº)</h4></figcaption></figure><h4 id=tp>TP</h4><p><strong>å¼ é‡å¹¶è¡Œï¼ˆTP, Tensor Parallelï¼‰</strong>ï¼šæ˜¯ä¸€ç§å±‚å†…å¹¶è¡Œï¼ˆIntra-Layer Parallelismï¼‰ç­–ç•¥ï¼Œå°†æ¨¡å‹ä¸­çš„ä¸€ä¸ªå±‚ï¼ˆå¦‚MLPå±‚ã€Attentionå±‚ï¼‰çš„å†…éƒ¨è®¡ç®—åˆ’åˆ†åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå¤šä¸ªè®¾å¤‡å…±åŒå®Œæˆè¯¥å±‚å‰å‘å’Œåå‘ä¼ æ’­ã€‚è¿™ä¹ˆåšå¯ä»¥çªç ´æ˜¾å­˜çš„é™åˆ¶ï¼Œä½†æ˜¯ä¼šå¯¹å»¶è¿Ÿè¾ƒæ•æ„Ÿã€‚</p><ul><li>Column-wise Parallelism (åˆ‡åˆ—ï¼Œç»´åº¦å®Œæ•´)</li></ul>$$W = [W_{1}, W_{2}] \newline Y_{i}=XW_{i}^{T}\ (on\ each\ GPU) \newline Y = Y_{1} + Y_{2}\ (AllReduce)$$<ul><li>Row-wise Parallelism (åˆ‡è¡Œ)</li></ul>$$W=\begin{bmatrix}W_{1} \\W_{2}\end{bmatrix} \newline Y_{i}=XW_{i}^{T}\ (on\ each \ GPU) \newline Y=concat(Y_{1}, Y_{2})\ (AllGather)$$<h4 id=pp>PP</h4><p><strong>æµæ°´çº¿å¹¶è¡Œï¼ˆPP, Pipeline Parallelï¼‰</strong>ï¼šæ˜¯ä¸€ç§å±‚é—´å¹¶è¡Œï¼ˆInter-Layer Parallelismï¼‰ç­–ç•¥ï¼Œå°†æ¨¡å‹æŒ‰é¡ºåºåˆ’åˆ†ä¸ºå¤šä¸ªstageï¼Œä¸åŒGPUæ‰§è¡Œä¸åŒçš„stageï¼Œå¤šä¸ªmicro-batchä»¥æµæ°´çº¿æ–¹å¼é€šè¿‡æ¨¡å‹ã€‚</p><p>æ”¯æŒæå¤§æ¨¡å‹ï¼ˆå±‚æ•°å¤šï¼‰ï¼Œæ˜¾å­˜éœ€æ±‚åˆ†å¸ƒåœ¨å„stageï¼Œä¸”è·¨GPUé€šä¿¡å‹åŠ›å°ï¼›ä½†æ˜¯å­˜åœ¨pipeline bubbleï¼ˆèµ·å§‹é˜¶æ®µGPUç©ºé—²ï¼Œå½±å“ååï¼‰</p><h2 id=quantization>Quantization</h2><h3 id=precision-formats>Precision Formats</h3><figure><img src=/p/llm3/img/3.png width='300px"'><figcaption><h4>TF32 strikes a balance that delivers performance with range and accuracy</h4></figcaption></figure><p>IEEE 754æ ‡å‡†ä¸­æµ®ç‚¹æ•°ç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šSç¬¦å·ä½ã€EæŒ‡æ•°ä½ã€Må°¾æ•°ä½ï¼Œæ¥ä¸‹æ¥ä»‹ç»å„ç§ç²¾åº¦æ ¼å¼ï¼š</p><ul><li><p><code>FP32</code> æ ‡å‡†çš„IEEE 754å•ç²¾åº¦æµ®ç‚¹æ ¼å¼ï¼Œ1ä½ç¬¦å·ä½+8ä½æŒ‡æ•°ä½+23ä½ä½æ•°ï¼ˆä¸‹æ–‡ç”¨[S, E, M]æ¥è¡¨ç¤ºï¼‰ï¼Œç²¾åº¦è¾ƒé«˜ï¼Œé€‚ç”¨äºæ‰€æœ‰ä¸»æµç¡¬ä»¶ï¼ˆCPUã€GPUã€TPUç­‰ï¼‰</p></li><li><p><code>TP32</code> NVIDIAåœ¨Ampereæ¶æ„å¼•å…¥çš„æ··åˆæ ¼å¼ï¼Œ[1, 8, 10]ï¼Œæˆªæ–­äº†å°¾æ•°ä½ï¼ˆå‡å°‘ä¹˜åŠ å¤æ‚åº¦ï¼‰ï¼Œæ”¯æŒTensor Coreä¼˜åŒ–ï¼Œç²¾åº¦ä»‹äºFP32å’ŒFP16ä¹‹é—´ï¼Œå¸¸åœ¨è®­ç»ƒæ—¶ä½œä¸ºFP32æ›¿æ¢</p></li><li><p><code>FP16</code> 16-bitåŠç²¾åº¦æµ®ç‚¹æ•°ï¼Œ[1, 5, 10]</p></li><li><p><code>BF16</code> Google TPUæ¨å‡ºçš„Brain Float 16ï¼Œ[1, 8, 7]ï¼Œå¸¸ç”¨äºæ··åˆç²¾åº¦è®­ç»ƒ</p></li><li><p><code>FP8</code> [1, 4, 3]æˆ–[1, 5, 2]ï¼Œéœ€è¦Hopperæ¶æ„GPUæ”¯æŒ</p></li><li><p><code>INT8</code> 8-bitæ•´å‹</p></li><li><p><code>FP4</code> [1, 2, 1]</p></li></ul><h2 id=reference>Reference</h2><p><a class=link href=https://zhuanlan.zhihu.com/p/662498827 target=_blank rel=noopener>å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿï¼šçœ‹å›¾å­¦KV Cache</a></p><p><a class=link href=https://zhuanlan.zhihu.com/p/659770503 target=_blank rel=noopener>LM(20)ï¼šæ¼«è°ˆ KV Cache ä¼˜åŒ–æ–¹æ³•ï¼Œæ·±åº¦ç†è§£ StreamingLLM</a></p><p><a class=link href=https://arxiv.org/abs/1911.02150 target=_blank rel=noopener>Fast Transformer Decoding: One Write-Head is All You Need</a></p><p><a class=link href=https://arxiv.org/abs/2305.13245 target=_blank rel=noopener>GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a></p><p><a class=link href=https://pytorch.ac.cn/tutorials/beginner/dist_overview.html target=_blank rel=noopener>PyTorch åˆ†å¸ƒå¼æ¦‚è§ˆ</a></p><p><a class=link href=https://arxiv.org/abs/2205.14135 target=_blank rel=noopener>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></p><p><a class=link href=https://arxiv.org/abs/2307.08691 target=_blank rel=noopener>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></p><p><a class=link href=https://arxiv.org/abs/2407.08608 target=_blank rel=noopener>FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</a></p><p><a class=link href=hhttps://arxiv.org/abs/1805.02867>Online normalizer calculation for softmax</a></p><p><a class=link href=https://zhuanlan.zhihu.com/p/638788074 target=_blank rel=noopener>ä¸€å¿ƒäºŒç”¨çš„Online Softmax</a></p></section><footer class=article-footer><section class=article-tags><a href=/tags/%E6%96%87%E6%A1%A3/>æ–‡æ¡£</a>
<a href=/tags/ai-infra/>AI Infra</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>ç›¸å…³æ–‡ç« </h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/llm2/><div class=article-image><img src=/p/llm2/img/cover.54ae66259e3c1b9a27ec946b3bb120cb_hu_253e638abf5259b5.png width=250 height=150 loading=lazy alt="Featured image of post æ‰‹æ“Transformerï¼šæ·±å…¥æ¶æ„ç»†èŠ‚" data-key=llm2 data-hash="md5-VK5mJZ48G5on7JRrO7Egyw=="></div><div class=article-details><h2 class=article-title>æ‰‹æ“Transformerï¼šæ·±å…¥æ¶æ„ç»†èŠ‚</h2></div></a></article><article class=has-image><a href=/p/llm1/><div class=article-image><img src=/p/llm1/img/cover.476a6e47d456e530769339129869e375_hu_f9f2be7966a8d7d9.png width=250 height=150 loading=lazy alt="Featured image of post ä»Transformerå¼€å§‹æ¢ç´¢BERT" data-key=llm1 data-hash="md5-R2puR9RW5TB2kzkSmGnjdQ=="></div><div class=article-details><h2 class=article-title>ä»Transformerå¼€å§‹æ¢ç´¢BERT</h2></div></a></article><article class=has-image><a href=/p/triton1/><div class=article-image><img src=/p/triton1/img/cover.67e39d5f04b0c7d5d295130692a51159_hu_f4abc157212c5d24.jpg width=250 height=150 loading=lazy alt="Featured image of post Tritonå­¦ä¹ â€”â€”Vector Addition, Fused Softmax, Matrix Multiplication" data-key=Triton1 data-hash="md5-Z+OdXwSwx9XSlRMGkqURWQ=="></div><div class=article-details><h2 class=article-title>Tritonå­¦ä¹ â€”â€”Vector Addition, Fused Softmax, Matrix Multiplication</h2></div></a></article><article class=has-image><a href=/p/hpc1/><div class=article-image><img src=/p/hpc1/img/cover.71e6162ba557f0a391bc5e9e5533f13f_hu_ea8068ab11796830.png width=250 height=150 loading=lazy alt="Featured image of post å¹¶å‘ç¯å¢ƒä¸‹çš„é˜Ÿåˆ—ä¼˜åŒ–â€”â€”æ— é”é˜Ÿåˆ—" data-key=hpc1 data-hash="md5-ceYWK6VX8KORvF6eVTPxPw=="></div><div class=article-details><h2 class=article-title>å¹¶å‘ç¯å¢ƒä¸‹çš„é˜Ÿåˆ—ä¼˜åŒ–â€”â€”æ— é”é˜Ÿåˆ—</h2></div></a></article><article class=has-image><a href=/p/hello-world/><div class=article-image><img src=/p/hello-world/img/cover.875f03a2ec7ba6cf35d6c32b0e811dab_hu_92077aa9cb038470.jpg width=250 height=150 loading=lazy alt="Featured image of post Hello World" data-key=hello-world data-hash="md5-h18Doux7ps811sMrDoEdqw=="></div><div class=article-details><h2 class=article-title>Hello World</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=KaigeZheng/KaigeZheng.github.io issue-term=pathname crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 Kambri's Blog</section><section class=powerby>ä½¿ç”¨ <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> æ„å»º<br>ä¸»é¢˜ <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> ç”± <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> è®¾è®¡</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>